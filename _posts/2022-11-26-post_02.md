---
title: "GPU Settings"
categories:
- Deep Learning
last_modified_at: 2022-11-26
toc: true
# layout: post
---

cuda device 설정을 통해 GPU 핸들링을 할 수 있다.
- 여러개의 gpu가 장착된 환경이라면, 적절하게 gpu를 핸들링하여 여러 딥러닝 모델 추론을 진행할 수 있음
- parameter size, input data size 가 너무 큰 경우 단일 gpu 로 OOM 발생 시, 여러 gpus parallel 사용하여 해결할 수 있다.

## 1. nvidia-smi

## 2. CUDA_VISIBLE_DEVICES
gpu_number 를 명시적으로 입력해 cuda 가 사용할 수 있는 gpu 를 제한시킬 수 있다.

- python_script.py (직접 실행의 경우)
    - CUDA_VISIBLE_DEVICE=2 python python_script.py
    - CUDA_VISIBLE_DEVICE=2,3 python python_script.py (parallel, 이후 DataParallel())

- python module 내부에서 실행 (os.environ)
    - os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID"  # Arrange GPU devices starting from 0
    - os.environ["CUDA_VISIBLE_DEVICE"] = "2"  # Set the GPU 2 to use
    - os.environ["CUDA_VISIBLE_DEVICE"] = "2,3,4"  # Set the GPUs 2,3,4 to use
